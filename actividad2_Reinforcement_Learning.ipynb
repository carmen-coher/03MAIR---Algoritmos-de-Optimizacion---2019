{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "actividad2_Reinforcement_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "myPRX3PnQ0P8"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carmen-herlo/03MAIR---Algoritmos-de-Optimizacion---2019/blob/master/actividad2_Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPZX8z5AS8G8",
        "colab_type": "text"
      },
      "source": [
        "# **ACTIVIDAD 2 - REINFORCEMENT LEARNING**\n",
        "\n",
        "Alumna: Carmen Hernández López\n",
        "\n",
        "URL: https://colab.research.google.com/drive/18GFaqq6ZZX0uxrYJ7EsjlpB4QynTXI7p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HdrcqX11Jna",
        "colab_type": "text"
      },
      "source": [
        "# **Teoría**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGKPtmDF1Pfu",
        "colab_type": "text"
      },
      "source": [
        "- Objetivo: Repasar los conceptos vistos en clase.\n",
        "- La puntuación de este bloque es de 4 puntos sobre la nota final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L1YIraL1P7A",
        "colab_type": "text"
      },
      "source": [
        "Define brevemente qué es el aprendizaje por refuerzo. ¿Qué diferencias hay entre aprendizaje supervisado, no supervisado y por refuerzo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PevwHaK41P-4",
        "colab_type": "text"
      },
      "source": [
        "### **Una breve introducción**\n",
        "\n",
        "La **minería de datos** es un campo del conocimiento que emplea la estadística y la ciencia de computación con la finalidad de, a través de técnicas de análisis de datos, tener un aprovechamiento posterior de dicha información.\n",
        "\n",
        "Dentro del ciclo de vida de la minería de datos se encuentra el **aprendizaje automático** (más comunmente denominado en su versión inglesa machine learning), estrechamente ligado con el análisis de datos y cuyo objetivo es el desarrollo de métodos que posibiliten que los ordenadores aprendan. Este *aprendizaje* trata la identificación y generalización de comportamientos observados en los datos.\n",
        "\n",
        "Existen tres grandes ramas de aprendizaje automático:\n",
        "\n",
        "- Aprendizaje por refuerzo: Basado en el concepto de prueba y error.\n",
        "- Aprendizaje supervisado: Enfocado a la predicción.\n",
        "- Aprendizaje no supervisado: En su mayoría descriptivo o de descubirimiento de la información.\n",
        "\n",
        "### **Las distintas ramas del aprendizaje automático**\n",
        "\n",
        "En el *aprendizaje por refuerzo* se tiene un agente con la capacidad de realizar una serie determinada de acciones dentro de un entorno. El agente realiza una serie de acciones que conforme se van sucediendo quedan actualizadas en un estado interno. El entorno devuelve una recompensa en función de la acción y ambas, recompensa y acción -entre otros datos-, quedan almacenadas en las transiciones, de tal forma que el agente puede aprender qué acciones o decisiones son mejores que otras: la función objetivo a maximizar es la recompensa. De esta forma, a medida que aumenta la recompensa, el agente va mejorando sus acciones o, lo que es lo mismo, se puede decir que el *agente está aprendiendo.*\n",
        "\n",
        "El *aprendizaje supervisado* parte de un conjunto de datos más sus etiquetas de tipo numérico o categórico. Mediante una técnica de aprendizaje entrenamops un regresor o un clasificador (dependiendo del tipo de etiqueta). El objetivo es, introduciendo una muestra -obviamente externa al conjunto de datos con el que se realizó el regresor/clasificador-, poder obtener una aproximación de la predicción.\n",
        "\n",
        "El *aprendizaje no supervisado* parte de un conjunto de datos con la peculiaridad de que carece de etiquetas. Al no existir etiquetas, no hay forma de comparar las predicciones, por tanto lo único que se puede medir es cómo de bueno es el agrupamiento realizado. Existen distintas técnicas de agrupamiento como el clustering. El objetivo es que los agrupamientos sean lo más compactos posibles (minimizar dispersión intracluster) y a su vez estén lo más separados posible de otros agrupamientos (minimizar dispersión intercluster).\n",
        "\n",
        "### **Enfrentando a las tres ramas**\n",
        "\n",
        "Se han explicado las tres ramas del aprendizaje por refuerzo a grandes pinceladas pero suficientes para observar la notoria diferencia entre los distintos tipos de aprendizaje. En resumidas cuentas, el aprendizaje supervisado trata de predecir y evaluar la bondad de la predicción, el aprendizaje no supervisado tiene como objetivo encontrar relaciones y patrones entre los atributos para realizar agrupaciones, y el aprendizaje por refuerzo se basa en entrenar a un agente a modo de prueba y error: comienza con acciones estocásticas y aquellas que le arrojen una mayor recompensa son aquellas a las que le otorgará una mayor importancia, de esta forma se irá reduciendo la aleatoriedad hasta poder aplicar acciones de un conocimiento aprendido."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teHu3fqp1QFD",
        "colab_type": "text"
      },
      "source": [
        "Define con tus palabras los conceptos de Entorno, Agente, Recompensa, Estado y Observación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IWuCiNS1QIK",
        "colab_type": "text"
      },
      "source": [
        "- **Entorno**: El mundo visual y lógico con el que interacciona el agente.\n",
        "- **Agente**: Entidad cuyo objetivo es maximizar la recompensa.\n",
        "- **Recompensa**: Feedback que devuelve el entorno: potencia o castiga acciones.\n",
        "- **Estado**: Pre-procesamiento de los datos.\n",
        "- **Observación**: Fotografía del entorno que se le pasa al agente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1by5pR2b1QLC",
        "colab_type": "text"
      },
      "source": [
        "Dependiendo del algoritmo de aprendizaje por refuerzo que se use, ¿qué clasificaciones podemos encontrar? Coméntalas brevemente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTZdNiw-1QOG",
        "colab_type": "text"
      },
      "source": [
        "**Clasificación de Problemas**:\n",
        "\n",
        "- **Basada en modelo**:\n",
        "    - **Model free**: No se conoce un modelo robusto para predecir la siguiente acción. Se basa en el estado actual. Es de carácter estocástico.\n",
        "    - **Model based**: Se conoce el modelo de reglas del juego. Se pueden adelantar varios movimientos. Se emplean algoritmos de optimización: ramificación y poda, estrategias metaheurísticas...\n",
        "    \n",
        "    \n",
        "- **Basada en estrategia**:\n",
        "    - **On policy**: La estrategia cambia con el tiempo. El alamacenamiento de N observaciones en un periodo t para actualizar modelo y aprendizaje del agente y así iterativamente. Se deshecha el paquete y vuelta a empezar. *Policy gradients* suele emplear este tipo de técnicas.\n",
        "    - **Off policy**: Se almacena la experiencia y transiciones desde el principio. Los paquetes de datos pueden ser versiones del modelo actual o más antiguo para poder obtener experiencias de todo tipo. Las *Q-Networks* suelen tener este tipo de comportamiento.\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gH-M5zd1QQ_",
        "colab_type": "text"
      },
      "source": [
        "Lista tres diferencias entre los algoritmos de DQN y Policy Gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfMHBIGS1QUL",
        "colab_type": "text"
      },
      "source": [
        "**DQN (Deep Q-Networks):**\n",
        "\n",
        "Basados en Q-learning, algoritmo de aprendizaje por refuerzo que se basa en el aprendizaje a partir de diferencias temporales. <<La información útil es la que se conoce en el estado actual>>. La estratrregia irá tomando la acción que maximiza el valor de la recompensa esperada. Se hace uso de un *discount factor* que estima la importancia que la recompensa tendrá en valores futuros. Se emplea la *ecuación de Bellman*, una fórmula recurrente que desarrolla el concepto de futuro de una manera finita y con la que podremos encontrar la fórmula óptima Q. <<La fórmula Q*(s,a) es igual a la suma de la recompensa inmediata, después de ejecutar la función A (acción) en el estado S (estado actual), y la recompensa futura esperada después de la transición al siguiente estado S>>. Se hará uso de CNNs como funciones aproximadoras en lugar de la tabla de q_values -cuya búsqueda exhaustiva de posiblidades hace intratable problemas complejos-. Con las CNN el proceso de aprendizaje se vuelve empírico y fundamenta las hipótesis en la \"prueba y error\". Es de necesidad una función de coste *loss* que mida el proceso de aprendizaje. Se busca minimizar el error en cada instante de tiempo que se produce entre Q(s, a) (término que se predice) y la ecuación de Bellmann (término objetivo). Para la convergencia serán elementos importantes la *target network* y el uso de una *secuencia de frames*.\n",
        "\n",
        "**Policy Gradient:**\n",
        "\n",
        "Como su propio nombre indica, se busca el gradiente de la *policy* (estrategia) para maximizar su valor. Se fundamenta en potenciar las acciones que nos devuelven una recompensa positiva y perjudicar las que nos devuelvan una recompensa negativa. En un estado cualquiera, el conjunto de acciones disponibles responderán a una distribución de probabilidades siguiendo la estrategia de recompensa positiva/negativa que se está aprendiendo (a excepción del estado inicial del que no se dispone información de estrategia). Para encontrar la estrategia óptima se hace uso de la *Cross-Entropy function*, que se basa en la ponderación de la propia recompensa de tomar una acción determinada. El factor Rt será el que determine la ponderación o los pesos en las acciones futuras con la finalidad de aumentar la recompensa. El número de *steps* o iteraciones escogido para el almacenamiento de la experiencia es esencial para el proceso de aprendizaje. Suele ser complejo encontrar correlación entre *estado* y *probabilidad de acción* dado a la gran varianza en los datos producida al usar la recompensa como factor de probabilidades de las acciones.\n",
        "\n",
        "**DQN vs Policy Gradient:**\n",
        "\n",
        "- DQN se emplea en problemas de estrategia **Off Policy** mientras Policy Gradient se emplea en problemas de estrategia **On Policy**. En caso de DQN (off policy) se tiene una estructura de datos global en el que se va almacenando la experiencia y a través de batchs se van actualizando los modelos. En el caso de Policy Gradient (on policy), toda la información adquirida para la actualización de los modelos en el proceso de aprendizaje se usa con una trayectoria cerrada de ejecuciones para ir actualizando los pesos e ir desechando toda la experiencia que se va acumulando. \n",
        "\n",
        "\n",
        "- DQN se centra en la relación estado-acción y mediante la función de coste a partir de la **ecuación de Bellman** trata, de manera recursiva y determiando a futuro las recompensas de los siguientes estados, cuál es la acción que maximiza el valor de la recompensa. Policy Gradient se centra en la estrategia, a través de la **función Cross-Entropy** se aplica una función de probabilidad a las acciones que varía en función de la obtención de recompensas positivas o negativas.\n",
        "\n",
        "\n",
        "- Policy Gradient tiene una gran adaptabilidad a una amplia gama de problemas y aprende en entornos estocásticos de una manera más sencilla que DQN, pero la **alta varianza** en los datos en Policy Gradient provocada por el uso de recompensa como factor puede conducir a la divergencia. Para paliar este comportamiento se emplean otras expresiones de Policy Gradient como por ejemplo la *función de ventaja* o *advantage function*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vyj9LOD1QXN",
        "colab_type": "text"
      },
      "source": [
        "# **Práctica**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUbUMDt81Qbs",
        "colab_type": "text"
      },
      "source": [
        "- Objetivo: Implementar una solución, usando _keras-rl_ y basada en el algoritmo de _DQN_ visto en clase, para que un agente aprenda una estrategia ganadora en el juego del _Pong_.\n",
        "\n",
        "- La puntuación de este bloque es de 6 puntos sobre la nota final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvKC0TBB2z0H",
        "colab_type": "text"
      },
      "source": [
        "El entorno sobre el que trabajaremos será _PongDeterministic-v0_ y el algoritmo que usaremos será _DQN_.\n",
        "\n",
        "Para evaluar cómo lo está haciendo el agente, la recompensa en el _Pong_ oscila, aproximadamente, en el rango de valores **[-20, 20]**. La estrategia óptima de un agente estaría alrededor de una media de recompensa de 20.\n",
        "\n",
        "- **NOTA IMPORTANTE**: Si el agente no llegara a aprender una estrategia ganadora, responder sobre la mejor estrategia obtenida."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myPRX3PnQ0P8",
        "colab_type": "text"
      },
      "source": [
        "## **Instalación de Tensorflow y Keras-rl**\n",
        "\n",
        "Serán necesarias para la realización de la actividad las versiones tensorflow-gpu.1.14.0 y keras-rl.0.4.2, así como el uso de la GPU -en este ejercicio se ha hecho uso de la GPU facilitada por Google Colab-."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jls07h913cwr",
        "colab_type": "code",
        "outputId": "4007322e-5111-4f57-e484-e9a3e4dadfab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "!pip install tensorflow-gpu==1.14"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 43kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.1.8)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.2.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.17.4)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 38.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.12.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 70.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (3.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.11.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.33.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (42.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (0.16.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14) (2.8.0)\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 1.14.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 1.14.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DapLhws0uM46",
        "colab_type": "code",
        "outputId": "efccbf79-eaf2-42e9-c450-f60a6e90f41e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!pip show tensorflow"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 1.15.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: termcolor, opt-einsum, google-pasta, wrapt, tensorboard, numpy, absl-py, six, gast, astor, keras-applications, tensorflow-estimator, grpcio, keras-preprocessing, protobuf, wheel\n",
            "Required-by: stable-baselines, magenta, fancyimpute\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS0eKWHqt04n",
        "colab_type": "code",
        "outputId": "b9af5132-7124-42d9-be7a-f734ecdff3a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('Not connected to TPU')\n",
        "else:\n",
        "  print('Connected to TPU')\n",
        "\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n",
            "Not connected to TPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Qv9UMsPwK4-",
        "colab_type": "code",
        "outputId": "6cb7e663-fbac-4765-f2aa-2913e3f34f40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.14.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FzDy9MXtm6G",
        "colab_type": "code",
        "outputId": "861b92b5-d9ae-4d6d-a66f-16b19ffa250a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "!pip install keras-rl==0.4.2"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-rl==0.4.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/87/4b57eff8e4bd834cea0a75cd6c58198c9e42be29b600db9c14fafa72ec07/keras-rl-0.4.2.tar.gz (40kB)\n",
            "\r\u001b[K     |████████                        | 10kB 30.1MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40kB 2.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl==0.4.2) (2.2.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.3.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl==0.4.2) (1.17.4)\n",
            "Building wheels for collected packages: keras-rl\n",
            "  Building wheel for keras-rl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-rl: filename=keras_rl-0.4.2-cp36-none-any.whl size=48379 sha256=703a104a0336fbbe90ca8fff18816519323844f45bd5a701f9587c74a62d1bfb\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/4d/84/9254c9f2e8f51865cb0dac8e79da85330c735551d31f73c894\n",
            "Successfully built keras-rl\n",
            "Installing collected packages: keras-rl\n",
            "Successfully installed keras-rl-0.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rl4B1cf4Sw_",
        "colab_type": "text"
      },
      "source": [
        "## **Base Folder en Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1NsJ1mkypGO",
        "colab_type": "code",
        "outputId": "a2754bdc-b893-451e-d1ae-00b047df227a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "###################################################################################################\n",
        "BASE_FOLDER = '/content/drive/My Drive/AI/redes_neuronales_y_deep_learning/actividad2_Reinforcement_Learning/'\n",
        "###################################################################################################\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDhrrr2KiZK0",
        "colab_type": "text"
      },
      "source": [
        "## **Librerías necesarias**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZlcG8ydwNc-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43b55df1-5796-4482-deb3-89deab40f3ac"
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute, MaxPooling2D, Dropout\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy, BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOtljLttirA9",
        "colab_type": "text"
      },
      "source": [
        "## **Entorno**\n",
        "\n",
        "Se definen a continuación el entorno 'PongDeterministic-v0'. Se tratra, como su nombre indica, de un entorno determinista: cada simulación del juego siempre comenzará de la misma manera. \n",
        "\n",
        "Cada simulación comienza de la misma manera en la misma secuencia de frames. Por tanto en nuestra experiencia almacenemos siempre los mismos estados y esto facilita el aprendizaje del agente. Por lo general los entornos deterministas facilitan el aprendizaje del agente y necesitan menos tiempos de entrenamiento.\n",
        "\n",
        "En el momento en el que se elimine la componente determinista y se pase a un entorno estocástico, el agente debería necesitar más tiempo y experiencia almacenada para alcanzar el mismo aprendizaje.\n",
        "\n",
        "Se puede crear un agente entrenado en un entorno determinista y testearlo con una versión estocástica con la finalidad de comprobar si el agente ha podido generalizar el comportamiento y es capaz de adaptarse. En caso afirmativo, habríamos conseguido un entrenamiento sin overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wz74G8ywSG0",
        "colab_type": "code",
        "outputId": "5f85ba64-711d-40a0-fe13-4667381db5c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# =============================================================================\n",
        "# ENTORNO\n",
        "# =============================================================================\n",
        "ENV_NAME = 'PongDeterministic-v0'\n",
        "\n",
        "# Get the environment and extract the number of actions.\n",
        "env = gym.make(ENV_NAME)\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "# random seed\n",
        "np.random.seed(123)\n",
        "env.seed(123)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[123, 151010689]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjd7QW-1nbCQ",
        "colab_type": "code",
        "outputId": "30605466-b5fd-4fc1-b628-d232640415c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('El número de acciones es {}'.format(nb_actions))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El número de acciones es 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CQLQGk3Te7r_"
      },
      "source": [
        "## **Procesamiento de las observaciones**\n",
        "\n",
        "Al usar keras-rl podemos ayudarnos de las preimplementaciones que tiene.\n",
        "\n",
        "En este caso, se hará uso de la clase Processor, la heredamos y le adaptamos una serie de métodos, con el fin de adaptar el código para una solución óptima del problema.\n",
        "\n",
        "Más adelante le pasaremos nuestra nueva clase al agente, el cual se encargará de implementarla. \n",
        "\n",
        "- **Método de procesamiento de información:**\n",
        "\n",
        "Cuando nos entra una observación del entorno comprobamos si tiene tres canales (altura, anchura, canales).\n",
        "\n",
        "Si se cumple, se aplican las distintas transformaciones necesarias para ajustarlos a los datos de entrada del modelo. \n",
        "\n",
        "En este caso realiza un resize a las dimensiones de nuestro INPUT_SHAPE y lo convertimos a escala de grises para convertirla en dos canales (altura y anchura de 84x84). \n",
        "\n",
        "Se pasa a un array de numpy, se comrpueba que las dimensiones coinciden con el INPUT_SHAPE y se convierte a un entero sin signo de 8 bits con la finalidad de ahorrar memoria. Se ha de tener cuidado con este casting y hacer la comprobación de si se pierde información o no, pues en caso de que así suceda, esto sería una mala práctica y habría que evitarla a toda costa.\n",
        "\n",
        " - **Normalización:**\n",
        "\n",
        "Como nuestras imágenes están ahora en escala de grises, las muestras tienen un valor entre 0 y 255. Se le aplica una normalización al batch de tal forma que los valores de cada píxel queden entre 0 y 1.\n",
        "\n",
        "- **Procesamiento de la recomensa:**\n",
        "\n",
        "La recompensa puede tener valores muy dispares. La recompensa es la que ayuda al algoritmo en el proceso de aprendizaje para ir midiendo cómo de buena es una acción o no. El hecho de que tenga valores muy altos no es aconsejable para guiar un proceso de entrenamiento, dado que las funciones de coste pueden adquirir órdenes de magnitud muy elevados y ésto complica la convergencia del proceso de aprendizaje. \n",
        "\n",
        "El rango en el que se mueve la recompensa es definido por nosotros. En este caso se le aplicará un numpy.clip que limita la recompensa entre valores [-1, 1]. Existen otros métodos como, por ejemplo, la estandarización por su media y desviación típica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vhyaHG-wlIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# =============================================================================\n",
        "# PROCESAMIENTO DE LAS OBSERVACIONES\n",
        "# =============================================================================\n",
        "\n",
        "# Define the input shape to resize the screen\n",
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "\n",
        "# This processor will be similar to the Atari processor\n",
        "class PongProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        \n",
        "        img = Image.fromarray(observation)\n",
        "        # resize and convert to grayscale\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        \n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')  # saves storage in experience memory\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fyck62GinR-p",
        "colab_type": "text"
      },
      "source": [
        "## **Arquitectura de la CNN**\n",
        "\n",
        "- **BASE MODEL:**\n",
        "\n",
        "Para nuestra Base Model se aplicarán tres bloques convolucionales. A mayor número de bloques, mayor profundidad de la red y más capacitada estará para extraer características locales -mayor precisión-.\n",
        "\n",
        "Para las capas se aplicarán filtros de depth semi-ascendente de 32 filtros, 64 filtros y 64 filtros, de tamaño 8x8, 4x4 y 3x3 con una variación 4x4, 2x2, 1x1 respectivamente. El sentido ascendente de los filtros tiene el sentido de extraer cada vez más información en x e y y almacenarla en z.\n",
        "\n",
        "Se hará uso de técnicas de pooling con la finalidad de reducir dimensionalidad sin perder información relevante.\n",
        "\n",
        "También se introducirá Dropout como medida para mitigar overfitting.\n",
        "\n",
        "- **TOP MODEL:**\n",
        "\n",
        "Se aplicará una capa flatten y capas densas -fullyconnected-, especificando en la última de las capas tantas neuronas como acciones existentes. La activación será 'Linear', que aportará el valor máximo esperado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71Nzxlkuw91t",
        "colab_type": "code",
        "outputId": "054d6c28-4cd5-4986-900f-ba9f26f45f23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "# =============================================================================\n",
        "# ARQUITECTURA CNN\n",
        "# =============================================================================\n",
        "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
        "\n",
        "# MODO SECUENCIAL\n",
        "model = Sequential()\n",
        "\n",
        "# cambio de orden de las dimensiones necesario para Tensorflow\n",
        "# (width, height, channels) orden de Tensorflow a permutar\n",
        "model.add(Permute((2, 3, 1), input_shape=input_shape)) \n",
        "\n",
        "# BLOQUE CONVOLUCIONAL 1 (BASE MODEL)\n",
        "model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# BLOQUE CONVOLUCIONAL 2 (BASE MODEL)\n",
        "model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(MaxPooling2D((2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# BLOQUE CONVOLUCIONAL 3 (BASE MODEL)\n",
        "model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# TOP MODEL\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "permute_2 (Permute)          (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 20, 20, 32)        8224      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 20, 20, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 10, 10, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 10, 10, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 4, 4, 64)          32832     \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 2, 2, 64)          36928     \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 2, 2, 64)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 512)               33280     \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 6)                 3078      \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 6)                 0         \n",
            "=================================================================\n",
            "Total params: 114,342\n",
            "Trainable params: 114,342\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmU7sxJSrT38",
        "colab_type": "text"
      },
      "source": [
        "## **Memoria y preprocesamiento:**\n",
        "\n",
        "Memoria secuencial: \n",
        "- Tamaño de memoria: 1 000 000\n",
        "- window_length: En este caso los 4 frames serán los elementos de la memoria que queremos tener como elementos de entrada en el estado del modelo.\n",
        "\n",
        "Preocesamiento:\n",
        "- Con la clase PongProcessor definida arriba."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtxT2yMj2BsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# =============================================================================\n",
        "# MEMORIA\n",
        "# =============================================================================\n",
        "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
        "\n",
        "# =============================================================================\n",
        "# PROCESAMIENTO\n",
        "# =============================================================================\n",
        "processor = PongProcessor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8ENrp-MsvnR",
        "colab_type": "text"
      },
      "source": [
        "## **Estrategia (policy)**\n",
        "\n",
        "Se hará uso de EpsGreedyQPolicy que emplea una estrategia voraz que se quedará con la mejor recompensa.\n",
        "\n",
        "Se empleará un epsilon comprendido entre 1 (exploración, acción siempre aleatoria) y 0.1 (explotación, acción la mayor´9ia de veces usando la policy aprendida).\n",
        "\n",
        "value_test=.05 determina el mínimo porcentaje de epsilon en test en el que el agente, en caso de quedar atrapado en bucle infinito y no saber como continuar, tener la opción de acciones aleatorias.\n",
        "\n",
        "nb_steps indica el proceso de exploración que en este caso será de 1 000 000 de steps. En este millón de steps, epsilon irá evolucionando desde un valor máximo de 1 a un valor mínimo de 0.1, que nos equilibrará el proceso de exploración y explotación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHwILsCU_HoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# =============================================================================\n",
        "# ESTRATEGIA\n",
        "# =============================================================================\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
        "                              nb_steps=1000000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NczhPIIDvak7",
        "colab_type": "text"
      },
      "source": [
        "## **Clase Agente de la DQN y compilación**\n",
        "\n",
        "- **Agente:**\n",
        "\n",
        "El agente requiere de model, nb_actions, policy y memory.\n",
        "Además introduciremos los parámetros:\n",
        "\n",
        "nb_steps_warmup: Indicará el almacenaje de las transiciones previas para almacenar experiencia. \n",
        "\n",
        "gamma: Un discount factor de 0.99 implica que se le otorga importancia a recompensas futuras.\n",
        "\n",
        "target_model_update: Actualización del target model cada 10 000 steps. Cada 10 000, vamos tomando los pesos del modelo y los pasamos a la target network.\n",
        "\n",
        "train_interval: Intervalo de entrenamiento cada 4 steps.\n",
        "\n",
        "- **Compilación:**\n",
        "\n",
        "Optimizador Adam con un learnig rate de 0.00025 y métrica Mean Absolute Error.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fo_QNljdAKX0",
        "colab_type": "code",
        "outputId": "cf26bbc7-6ecf-4cbb-805d-89e75a2eadb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# =============================================================================\n",
        "# AGENTE\n",
        "# =============================================================================\n",
        "\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
        "               processor=processor, nb_steps_warmup=50000, gamma=.99, target_model_update=10000,\n",
        "               train_interval=4, delta_clip=1.)\n",
        "\n",
        "# =============================================================================\n",
        "# COMPILACIÓN\n",
        "# =============================================================================\n",
        "optimizer = Adam(lr=.00025)\n",
        "dqn.compile(optimizer, metrics=['mae'])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/rl/util.py:79: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euz1drVIx8eu",
        "colab_type": "text"
      },
      "source": [
        "## **Funciones callback**\n",
        "\n",
        "Controlan el proceso de entrenamiento.\n",
        "\n",
        "Introducimos un callback del checkpoint del modelo de tal forma que cuando el resultado que se esté obteniendo sea mejor al de las métricas anteriores se hace automaticamente un checkpoint, aunque con el parámetro *interval* le podemos indicar cada cuánto queremos que se realice el checkpoint.\n",
        "\n",
        "Agregamos un logger que recoge toda la información del entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxC2YNMCxx1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# =============================================================================\n",
        "# CALLBACKS\n",
        "# =============================================================================\n",
        "weights_filename = BASE_FOLDER + 'dqn_{}_weights.h5f'.format(ENV_NAME)\n",
        "checkpoint_weights_filename = BASE_FOLDER + 'dqn_' + ENV_NAME + '_weights_{step}.h5f'\n",
        "log_filename = BASE_FOLDER + 'dqn_{}_log.json'.format(ENV_NAME)\n",
        "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
        "callbacks += [FileLogger(log_filename, interval=100)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTq2_SjYy4J5",
        "colab_type": "text"
      },
      "source": [
        "## **Entrenamiento**\n",
        "\n",
        "- **FIT:**\n",
        "\n",
        "env: Se le pasa el entorno.\n",
        "\n",
        "callbacks: se le pasa como parámetro los callbacks que controlan el proceso de almacenamiento.\n",
        "\n",
        "nb_steps: Se trata del número de steps que queremos ejecutar en el entrenamiento, en este caso de 1 750 000. De éstos, el primer millón van a ser de exploración decreciendo el valor de epsilon de 1 a 0.1 (se ha definido antes).\n",
        "\n",
        "log_interval: Indica el intervalo de loggeo en steps.\n",
        "\n",
        "verbose: 0 para no loggear, 1 para loggear por intervalo (log_interval), 2 para loggear por episodio.\n",
        "\n",
        "- **SAVE_WEIGHTS:**\n",
        "\n",
        "Guarda los pesos del agente como un archivo HDF5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JtPa3A9AnYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# =============================================================================\n",
        "# ENTRENAMIENTO\n",
        "# =============================================================================\n",
        "nb_steps = 1750000\n",
        "\n",
        "#comentar si una vez guardados los pesos sólo se desea testear\n",
        "dqn.fit(env, callbacks=callbacks, nb_steps=nb_steps, verbose=1, log_interval=nb_steps/100) #10000) #nb_steps/10)\n",
        "dqn.save_weights(weights_filename, overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgwDeSU41qth",
        "colab_type": "text"
      },
      "source": [
        "## **Testeo**\n",
        "\n",
        "Evaluar el algoritmo para un número de episodios.\n",
        "\n",
        "Cargamos los pesos del modelo en nuestro agente mediante load_weights y hacemos un testing en este caso de 10 episodios para evaluar cómo se comporta.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xEw37KDB7eb",
        "colab_type": "code",
        "outputId": "eb4581bd-0f1d-4c09-af53-ffa1e56c214f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# =============================================================================\n",
        "# TESTEO\n",
        "# =============================================================================\n",
        "weights_filename = BASE_FOLDER + 'dqn_{}_weights.h5f'.format(ENV_NAME)\n",
        "dqn.load_weights(weights_filename)\n",
        "\n",
        "nb_episodes = 10\n",
        "dqn.test(env, nb_episodes=nb_episodes, visualize=False)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: -21.000, steps: 764\n",
            "Episode 2: reward: -21.000, steps: 764\n",
            "Episode 3: reward: -21.000, steps: 764\n",
            "Episode 4: reward: -21.000, steps: 764\n",
            "Episode 5: reward: -21.000, steps: 764\n",
            "Episode 6: reward: -21.000, steps: 764\n",
            "Episode 7: reward: -21.000, steps: 764\n",
            "Episode 8: reward: -21.000, steps: 764\n",
            "Episode 9: reward: -21.000, steps: 764\n",
            "Episode 10: reward: -21.000, steps: 764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f39e02b5978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8oVggYTJ1SX",
        "colab_type": "text"
      },
      "source": [
        "## **Evaluación de los resultados y conclusión**\n",
        "\n",
        "Tras el entrenamiento y un testeo de 10 partidas el agente obtiene la puntuación mínima. El agente no ha aprendido. Mediante la técnica DQN, la librería keras-rl y tensorflow y el empleo de CNNs como función de aproximación, no se ha logrado obtener resultados satisfactorios para el aprendizaje de nuestro agente.\n",
        "\n",
        "La red convolucional es una red sencilla a la cual se le han introducido parámetros sencillos de reducción de dimensionalidad sin la pérdida de información valiosa para facilitar el entrenamiento y la rapidez en la convergencia.\n",
        "\n",
        "Se ha hecho uso de callbacks para guardar los pesos y poder volver a entrenar cargando pesos sin la necesidad de empezar desde cero.\n",
        "\n",
        "Se ha hecho uso de las librerías necesarias para la compatiblidad de las técnicas eneseñadas en las clases (tensorflow-gpu.1.14 y keras.0.4.2), además de hacer uso de la GPU proporcionada por Google Colab.\n",
        "\n",
        "Se ha hecho uso de un código en Java a introducir en la consola del explorador Chrome para evitar la desconexión de Google Colab tras un tiempo establecido por defecto -definido en el apartado *Páginas de Interés*-.\n",
        "\n",
        "Se ha observado durante el entrenamiento una media de -21 de puntuación y valores mínimos de -21 y máximo comprendidos entre -21 y -18. El caso de observar, en ocasiones, valores superiores a -21 no implican aprendizaje en el agente, sino que se trata de que por aleatoriedad y probabilidad se puede obtener una mejor puntuación. Un aprendizaje del agente se vería reflejado en una mejoría de la puntuación progresiva, nunca aleatoria o estocástica.\n",
        "\n",
        "Se llega a la conclusión de que en definitiva, el uso de DQNs las cuales implican uso de CNNs para problemas que adquieren cierta complejidad, requieren no sólo del conocimiento en profundidad de este tipo de técnicas, sino que además es necesario y fundamental contar con el **equipo hardware** y el **tiempo** -se hace énfasis en el coste computacional temporal- que el problema en cuestión, por su complejidad, necesite para el aprendizaje del agente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc_Hbuw_6_DX",
        "colab_type": "text"
      },
      "source": [
        "## **Páginas de interés**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU03nPXWJxGm",
        "colab_type": "text"
      },
      "source": [
        "Las siguientes páginas han sido de gran ayuda para la realización de la práctica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Cndrgqvx9iA",
        "colab_type": "text"
      },
      "source": [
        "https://medium.com/@mukesh.kumar43585/model-checkpoint-google-colab-and-drive-as-persistent-storage-for-long-training-runs-e35ffa0c33d9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ0P_bL3QTwi",
        "colab_type": "text"
      },
      "source": [
        "https://nextjournal.com/a/C7pDhirqsdaCp7LVsizVtX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzE7V8e33CBN",
        "colab_type": "text"
      },
      "source": [
        "https://stackoverflow.com/questions/57113226/how-to-prevent-google-colab-from-disconnecting\n",
        "\n",
        "Código Java a introducir en consola Chrome para evitar desconexión por defecto:\n",
        "\n",
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\"); \n",
        "    document.querySelector(\"#connect\").click() \n",
        "}\n",
        "setInterval(ConnectButton,60000);\n"
      ]
    }
  ]
}